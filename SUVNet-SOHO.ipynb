{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6871c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random as rn\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#DNN\n",
    "from tensorflow.keras.layers import Lambda, Concatenate, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "from numpy import load\n",
    "import pickle\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from metrics import save_metrics_iiet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"libraries loaded\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d263fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_riproducibility(seed):\n",
    "   \n",
    "    #set seed \n",
    "\n",
    "    #Python SEED\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    #numpy seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #tf seed\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    #rn seed\n",
    "    rn.seed(seed)\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    \n",
    "    #\n",
    "    from tensorflow.compat.v1.keras import backend as K\n",
    "    \n",
    "    #sess\n",
    "    sess = tf.compat.v1.get_default_session()\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    #tf seed\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    \n",
    "    #os.environ['KERAS_BACKEND'] = \"tensorflow\"\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_logvar = args\n",
    "    eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return eps * tf.exp(z_logvar * .5) + z_mean\n",
    "\n",
    "\n",
    "def init_sparse_vae(params):\n",
    "    input_dim = params[\"input_dimension\"]\n",
    "    output_dim = input_dim\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    loss = params[\"loss\"]\n",
    "    inner_seed = params[\"seed\"]\n",
    "    \n",
    "    # this is our input placeholder\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # encoding phase\n",
    "    sparse = tf.keras.layers.Dense(1024, activation='relu')(input_layer)    \n",
    "    encoded_l0 = tf.keras.layers.Dense(512, activation='relu')(sparse)    # 256\n",
    "    \n",
    "    z_mean = tf.keras.layers.Dense(256, name=\"z_mean\")(encoded_l0) #8\n",
    "    z_logvar = tf.keras.layers.Dense(256, name=\"z_log_var\")(encoded_l0)\n",
    "    latent_space = tf.keras.layers.Lambda(sampling, output_shape=(256,))([z_mean, z_logvar])\n",
    "\n",
    "    # decoding phase \n",
    "    \n",
    "    decoded_l0 = tf.keras.layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "    \n",
    "    l0_in = tf.keras.layers.Concatenate()([encoded_l0, decoded_l0])\n",
    "    sparse_dec = tf.keras.layers.Dense(1024, activation=\"relu\")(l0_in)\n",
    "    \n",
    "    l_in = tf.keras.layers.Concatenate()([sparse, sparse_dec])\n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation=\"sigmoid\")(l_in)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = tf.keras.models.Model(input_layer, output_layer, name = 'SparseVAE')\n",
    "    model.summary()\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mean_squared_error(input_layer, output_layer)\n",
    "    kl_loss = -0.5 * tf.reduce_mean((0.5 * z_logvar - tf.exp(0.5 * z_logvar) - tf.square(z_mean) + 1.))\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    model.add_loss(vae_loss)\n",
    "    model.add_metric(kl_loss, name=\"kl_loss\")\n",
    "    model.add_metric(reconstruction_loss, name=\"reconstruction_loss\")\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "def build_detector(data, columns_names, params, model_selector, model_path):\n",
    "\n",
    "    view = data[columns_names]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    num_epoch = params[\"num_epoch\"]\n",
    "    verbose = params[\"verbose_output\"]\n",
    "    print(\"Model Selector \"+str(model_selector))\n",
    "    \n",
    "    if model_selector == 0:\n",
    "        model = init_vae(params)\n",
    "    elif model_selector == 1:\n",
    "        model = init_deep_ae(params)\n",
    "    elif model_selector == 2:\n",
    "        model = init_sparse_ae(params)\n",
    "    elif model_selector == 3:\n",
    "        model =  init_unet(params)\n",
    "    elif model_selector == 4:\n",
    "        model =  init_sparse_vae(params)\n",
    "    else:\n",
    "        print('ERROR: NO MODEL')\n",
    "        sys.exit()\n",
    "                    \n",
    "    print(\"best model path: \", model_path)\n",
    "    check = ModelCheckpoint(model_path, monitor='loss', verbose=2, save_best_only=True, save_weights_only=True, mode='min')\n",
    "  \n",
    "    history = model.fit(view, view, batch_size=batch_size, epochs=num_epoch, verbose=verbose, callbacks=[check])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further models\n",
    "def init_deep_ae(params):\n",
    "    \n",
    "    #read params\n",
    "    \n",
    "    input_dim = params[\"input_dimension\"]\n",
    "    output_dim = input_dim\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    loss = params[\"loss\"]\n",
    "    inner_seed = params[\"seed\"]\n",
    "    \n",
    "    # this is our input placeholder\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # encoding phase\n",
    "    encoded_l0 = tf.keras.layers.Dense(256, activation='relu')(input_layer)    \n",
    "    encoded_l1 = tf.keras.layers.Dense(128, activation='relu')(encoded_l0) \n",
    "    encoded_l2 = tf.keras.layers.Dense(64, activation='relu')(encoded_l1)\n",
    "    \n",
    "    latent_space = tf.keras.layers.Dense(32, activation='relu')(encoded_l2)\n",
    "\n",
    "    \n",
    "    # decoding phase \n",
    "    decoded_l2 = tf.keras.layers.Dense(64, activation=\"relu\")(latent_space)\n",
    "    decoded_l1 = tf.keras.layers.Dense(128, activation=\"relu\")(decoded_l2)\n",
    "    decoded_l0 = tf.keras.layers.Dense(256, activation=\"relu\")(decoded_l1)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation=\"sigmoid\")(decoded_l0)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = tf.keras.models.Model(input_layer, output_layer, name = 'DAE')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_sparse_ae(params):\n",
    "    \n",
    "    #read params\n",
    "    \n",
    "    input_dim = params[\"input_dimension\"]\n",
    "    output_dim = input_dim\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    loss = params[\"loss\"]\n",
    "    inner_seed = params[\"seed\"]\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # encoding phase    \n",
    "    latent_space = tf.keras.layers.Dense(1024, activation='relu')(input_layer)\n",
    "    \n",
    "    # decoding phase \n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation=\"sigmoid\")(latent_space)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = tf.keras.models.Model(input_layer, output_layer, name = 'SAE')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def init_unet(params):\n",
    "    \n",
    "    #read params\n",
    "    \n",
    "    input_dim = params[\"input_dimension\"]\n",
    "    output_dim = input_dim\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    loss = params[\"loss\"]\n",
    "    inner_seed = params[\"seed\"]\n",
    "    #reg_value = params[\"reg_value\"]\n",
    "    \n",
    "    \n",
    "    # this is our input placeholder\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # encoding phase\n",
    "    encoded_l0 = tf.keras.layers.Dense(256, activation='relu')(input_layer)    \n",
    "    encoded_l1 = tf.keras.layers.Dense(128, activation='relu')(encoded_l0)    \n",
    "    encoded_l2 = tf.keras.layers.Dense(64, activation='relu')(encoded_l1)\n",
    "    \n",
    "    latent_space = tf.keras.layers.Dense(32, activation='relu')(encoded_l2)\n",
    "    \n",
    "    # decoding phase \n",
    "    decoded_l2 = tf.keras.layers.Dense(64, activation=\"relu\")(latent_space)\n",
    "    \n",
    "    l1_in = tf.keras.layers.Concatenate()([decoded_l2, encoded_l2])\n",
    "    decoded_l1 = tf.keras.layers.Dense(128, activation=\"relu\")(l1_in)\n",
    "    \n",
    "    l0_in = tf.keras.layers.Concatenate()([decoded_l1, encoded_l1])\n",
    "    decoded_l0 = tf.keras.layers.Dense(256, activation=\"relu\")(l0_in)\n",
    "    \n",
    "    out_in = tf.keras.layers.Concatenate()([decoded_l0, encoded_l0])\n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation=\"sigmoid\")(out_in)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = tf.keras.models.Model(input_layer, output_layer, name = 'UNET')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def init_vae(params):\n",
    "    input_dim = params[\"input_dimension\"]\n",
    "    output_dim = input_dim\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    loss = params[\"loss\"]\n",
    "    inner_seed = params[\"seed\"]\n",
    "    \n",
    "    # this is our input placeholder\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # encoding phase\n",
    "    encoded_l0 = tf.keras.layers.Dense(512, activation='relu')(input_layer)    \n",
    "    \n",
    "    z_mean = tf.keras.layers.Dense(256, name=\"z_mean\")(encoded_l0)\n",
    "    z_logvar = tf.keras.layers.Dense(256, name=\"z_log_var\")(encoded_l0)\n",
    "    latent_space = tf.keras.layers.Lambda(sampling, output_shape=(256,))([z_mean, z_logvar])\n",
    "\n",
    "    # decoding phase \n",
    "    decoded_l0 = tf.keras.layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(output_dim, activation=\"sigmoid\")(decoded_l0)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = tf.keras.models.Model(input_layer, output_layer, name = 'VAE')\n",
    "    model.summary()\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mean_squared_error(input_layer, output_layer)\n",
    "    kl_loss = -0.5 * tf.reduce_mean((0.5 * z_logvar - tf.exp(0.5 * z_logvar) - tf.square(z_mean) + 1.))\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    model.add_loss(vae_loss)\n",
    "    model.add_metric(kl_loss, name=\"kl_loss\")\n",
    "    model.add_metric(reconstruction_loss, name=\"reconstruction_loss\")\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_more_feat(d, columns, add_to_list=False):\n",
    "    \n",
    "    range_values = [2, 4, 8, 16]\n",
    "    \n",
    "    list_copy = columns.copy()\n",
    "    \n",
    "    for c in columns:\n",
    "        \n",
    "        d[\"one_minus_\"+c] = 1 - d[c].clip(0,1) \n",
    "        \n",
    "        #power \n",
    "        for v in range_values:\n",
    "            d[\"power_\"+str(v)+\"_\"+c] = d[c]**v\n",
    "        \n",
    "        #root\n",
    "        for v in range_values:\n",
    "            d[\"root_\"+str(v)+\"_\"+c] = d[c].clip(0,1)**(1/v)\n",
    "        \n",
    "        #sin\n",
    "        d[\"sin_\"+c] = np.sin(math.pi * d[c].clip(0,1))\n",
    "        \n",
    "        #log\n",
    "        d[\"log_\"+c] = np.log((d[c].clip(0,1)+1)/math.log(2))\n",
    "            \n",
    "        #exp\n",
    "        d[\"exp_\"+c] = np.exp(d[c]-1)\n",
    "        \n",
    "        if add_to_list:\n",
    "            \n",
    "            list_copy.append(\"one_minus_\"+c)\n",
    "            \n",
    "            for v in range_values:\n",
    "                list_copy.append(\"power_\"+str(v)+\"_\"+c)\n",
    "            for v in range_values:\n",
    "                list_copy.append(\"root_\"+str(v)+\"_\"+c)   \n",
    "\n",
    "            list_copy.append(\"sin_\"+c)\n",
    "            list_copy.append(\"log_\"+c)\n",
    "            list_copy.append(\"exp_\"+c)\n",
    "                   \n",
    "    return d, list_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a8869-8e71-46d8-ab22-c0c03beebf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_percentage = 100\n",
    "model_ = 'SUVNet'\n",
    "dataset = 'SOHO'\n",
    "model_name = f\"model_{model_}_training_perc_{str(training_percentage)}_dataset_{dataset}\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003608b-fce4-4ccd-9a3e-de48de029f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./output/model/\"+model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c65450",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET PARAMETERS\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 23071983\n",
    "np.random.seed(seed)\n",
    "\n",
    "to_remove = ['flow', 'index', 'src', 'dst', 'src_port', 'dst_port', 'protocol', 'flowstarttime', 'flowendtime', 'label']\n",
    "\n",
    "VERBOSE_OUTPUT = 1\n",
    "\n",
    "ADD_FEATURES = 1\n",
    "DROP_REDUNDANT_FEATURES = 1  \n",
    "ADD_NOISE = 0\n",
    "ADD_BATCH = 0 #FIXED\n",
    "num_epochs = 250\n",
    "batch_size = 1024\n",
    "\n",
    "work_area_data = './dataset/SOHO'\n",
    "\n",
    "#Read Training set\n",
    "\n",
    "# Read the training set\n",
    "training = pd.read_csv(os.path.join(work_area_data, 'training_set.csv'))\n",
    "print(training.shape)\n",
    "print(training.columns)\n",
    "training.describe()\n",
    "\n",
    "#define feat\n",
    "columns_of_interest = [x for x in training.columns if x not in to_remove] \n",
    "#initial feat set\n",
    "original_feat = columns_of_interest.copy()\n",
    "print(columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39643572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop correlate feats\n",
    "\n",
    "if DROP_REDUNDANT_FEATURES:\n",
    "    cor_matrix = training.drop(columns=to_remove).corr().abs()\n",
    "    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "    #print(upper_tri)\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "    print()\n",
    "    print(\"number to drop:\", len(to_drop))\n",
    "    print(\"to drop: \", to_drop)\n",
    "\n",
    "    #update feats\n",
    "    columns_of_interest = [x for x in columns_of_interest if x not in to_drop]\n",
    "    original_feat = columns_of_interest.copy()\n",
    "\n",
    "    print(\"Final Columns:\", columns_of_interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eaf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and apply scaler on training set\n",
    "scaler_x =  MinMaxScaler(feature_range=(0, 1)) #MinMaxScaler(feature_range=(0, 1)) # StandardScaler()\n",
    "scaler_x.fit(training[columns_of_interest])\n",
    "training[columns_of_interest] = scaler_x.transform(training[columns_of_interest])\n",
    "print(training[columns_of_interest].shape)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add further feat\n",
    "if ADD_FEATURES:\n",
    "    training, columns_of_interest = add_more_feat(training, columns_of_interest, True)\n",
    "\n",
    "    print(len(columns_of_interest))\n",
    "    print(training.shape)\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training[columns_of_interest].shape)\n",
    "print(training[columns_of_interest].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1356394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdaffb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"input dim: \", len(columns_of_interest))\n",
    "\n",
    "#init params\n",
    "reconstructor_params = {\"input_dimension\": len(columns_of_interest),\n",
    "                        \"batch_size\" : batch_size,\n",
    "                        \"num_epoch\" : num_epochs,\n",
    "                        \"verbose_output\" : VERBOSE_OUTPUT,\n",
    "                        \"optimizer\":'adam',\n",
    "                        \"seed\": seed,\n",
    "                        \"use_bn\": ADD_BATCH,\n",
    "                        \"add_noise\": ADD_NOISE,\n",
    "                        #\"reg_value\": 10e-7,\n",
    "                        \"loss\":\"mse\"}\n",
    "\n",
    "\n",
    "#refresh ripr.\n",
    "refresh_riproducibility(seed)\n",
    "\n",
    "model_id = 4\n",
    "\n",
    "# starts training phase\n",
    "\n",
    "start_time = int(round(time.time() * 1000))\n",
    "detector, history = build_detector(training, columns_of_interest, reconstructor_params, model_id, model_path)\n",
    "end_time = int(round(time.time() * 1000))\n",
    "total_time = end_time - start_time\n",
    "print('Total Learning Time:'+str(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b53858-b853-4f9d-9b7c-798c69046890",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history.history)\n",
    "his = history.history['reconstruction_loss']\n",
    "df_his = pd.DataFrame(data=his, columns=[\"Loss\"])\n",
    "df_his['epoch'] = range(1, len(df_his) + 1)\n",
    "print(df_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa05b72-6e6c-4db8-be0a-008ea1b3fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save History\n",
    "file_history = \"./output/history/history_\"+model_name\n",
    "print(file_history)\n",
    "np.save(file_history, history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba69df-e52c-4a11-8d52-cc6ba98c8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_history = \"./output/history/history_\"+model_name\n",
    "history_read=np.load(file_history+'.npy', allow_pickle='TRUE').item()\n",
    "his = history_read['reconstruction_loss']\n",
    "df_his = pd.DataFrame(data=his, columns=[\"Loss\"])\n",
    "df_his['epoch'] = range(1, len(df_his) + 1)\n",
    "df_his['Model']='SUVNet' \n",
    "\n",
    "plt.plot(df_his['epoch'], df_his['Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59953782-1142-46b3-9fa9-6128f1e1b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_his['epoch'], df_his['Loss'])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9b446-87b2-4927-9562-2a0184f7e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input dim: \", len(columns_of_interest))\n",
    "\n",
    "#init params\n",
    "reconstructor_params = {\"input_dimension\": len(columns_of_interest),\n",
    "                        \"batch_size\" : batc_size,\n",
    "                        \"num_epoch\" : num_epochs,\n",
    "                        \"verbose_output\" : VERBOSE_OUTPUT,\n",
    "                        \"optimizer\":'adam',\n",
    "                        \"seed\": seed,\n",
    "                        \"use_bn\": ADD_BATCH,\n",
    "                        \"add_noise\": ADD_NOISE,\n",
    "                        #\"reg_value\": 10e-7,\n",
    "                        \"loss\":\"mse\"}\n",
    "\n",
    "#refresh ripr.\n",
    "refresh_riproducibility(seed)\n",
    "\n",
    "detector =  init_sparse_vae(reconstructor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50b0b7-306c-4aa7-b5b4-e25fcc127d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload best model\n",
    "detector.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79aacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute reconstruction\n",
    "training_predictions = detector.predict(training[columns_of_interest], batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496388b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute outlierness on training\n",
    "outlierness_training = np.sum(np.power(np.absolute(training_predictions - training[columns_of_interest]), 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(detector, file_path, threshold, attack_type=None, th=None, model='VAE'):\n",
    "    print(f'Attack type: {attack_type}')\n",
    "    print(f'Threshold: {threshold}')\n",
    "    \n",
    "    # Read the test set\n",
    "    testset = pd.read_csv(file_path)\n",
    "    \n",
    "    #extract class\n",
    "    y_test = testset[\"class\"]\n",
    "    print(y_test)\n",
    "    \n",
    "    #compute scaling\n",
    "    testset[original_feat] = scaler_x.transform(testset[original_feat])\n",
    "    \n",
    "    #add derived feat\n",
    "    if ADD_FEATURES:\n",
    "        testset, _ = add_more_feat(testset, original_feat, False)\n",
    "        print(\"done\")\n",
    "\n",
    "    # print(testset.columns)\n",
    "    \n",
    "    detector.load_weights(model_path)\n",
    "    \n",
    "    #compute reconstructions\n",
    "    start_time = int(round(time.time() * 1000))\n",
    "\n",
    "    predictions = detector.predict(testset[columns_of_interest], batch_size=4096)\n",
    "\n",
    "    end_time = int(round(time.time() * 1000))\n",
    "    total_time = end_time - start_time\n",
    "    #print(start_time)\n",
    "    #print(end_time)\n",
    "    print('Total Prediction Time:'+str(total_time))\n",
    "    print('Single Prediction Time:'+str(total_time/testset.shape[0]))\n",
    "    \n",
    "    #compute outlierness\n",
    "    outlierness = np.sum(np.power(np.absolute(predictions - testset[columns_of_interest]), 1), axis=1)\n",
    "    \n",
    "    #compute prediction\n",
    "    y_pred = outlierness > threshold\n",
    "\n",
    "    #debug: num of yes\n",
    "    np.sum(y_pred)\n",
    "\n",
    "    report_map = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(report_map)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    result = str(acc_score)+\";\"+str(report_map['1.0']['precision']) + \";\" + str(report_map['1.0']['recall']) + \";\" + str(report_map['1.0']['f1-score']) \n",
    "    print(\"acc;prec;rec;f1\")\n",
    "    print(result)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    far = cm[0][1]/(cm[0][1]+cm[1][1])\n",
    "    print(\"FAR: \", far)\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, outlierness)\n",
    "    print(\"AUC: \", auc_score)\n",
    "\n",
    "    pr1, rec1, thr1 = precision_recall_curve(y_test, outlierness)\n",
    "    auc_score_pr = auc(rec1,pr1)\n",
    "    print(\"AUC-PR: \", auc_score_pr)\n",
    "    \n",
    "    import imblearn as imb\n",
    "    from imblearn.metrics import geometric_mean_score\n",
    "    g_mean = str(round(geometric_mean_score(y_test, y_pred, average = 'binary'), 3))\n",
    "    print((\"G-Mean: \", g_mean))\n",
    "\n",
    "    return outlierness, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean = np.mean(outlierness_training)\n",
    "_std = np.std(outlierness_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20715547-fb9b-4591-8258-392540570688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SparseVAE'\n",
    "attack_type = 'slowDoS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed98e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.5\n",
    "threshold = _mean + alpha * _std\n",
    "test(detector, os.path.join(work_area_data, 'test_soho.csv'), threshold, attack_type, th=str(alpha), model=model_name)\n",
    "\n",
    "alpha = 2.75\n",
    "threshold = _mean + alpha * _std\n",
    "test(detector, os.path.join(work_area_data, 'test_soho.csv'), threshold, attack_type, th=str(alpha), model=model_name)\n",
    "\n",
    "alpha = 3.\n",
    "threshold = _mean + alpha * _std\n",
    "test(detector, os.path.join(work_area_data, 'test_soho.csv'), threshold, attack_type, th=str(alpha), model=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ee1e1-c853-4a71-bab4-ac6aed7df37b",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
